{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52290b6d",
   "metadata": {},
   "source": [
    "# Homework 2.0: Backpropagation\n",
    "\n",
    "Homework 2 has several parts.  In this first part, we will work with backpropagation.\n",
    "\n",
    "## Learning Objective\n",
    "\n",
    "In this part of the homework you will implement the core part of the backpropagation algorithm by hand and understand its use of memory and time.\n",
    "\n",
    "## Readings\n",
    "\n",
    "[Rumelhart, Hinton and Williams, *Learning representations\n",
    "by back-propagating errors*, 1986](https://papers.baulab.info/Rumelhart-1986.pdf) - Very influential in popularizing backpropagation, also known to numerical analysts as \"reverse mode autodifferentiation\".\n",
    "\n",
    "[Andreas Griewank, *On automatic differentiation and algorithmic linearization*, 2014](https://papers.baulab.info/also/Griewank-2014.pdf) - A mathematical presentation of both the forward and reverse-mode algorithm.  Since in deep network training we tpically have a single-dimensional scalar loss, we only focus on reverse-mode, which is far more efficient for our case.  May be helpful in the axioms and examples it provides.\n",
    "\n",
    "[Andreas Griewank, *Who invented the reverse mode of differentiation?*, 2012](https://papers.baulab.info/also/Griewank-2012.pdf) - Of historical interest.\n",
    "\n",
    "## Exercise 2.0.1: Implement backpropagation on paper\n",
    "\n",
    "Backpropagation is based on the idea of computing gradients efficently by remembering and reusing partial results.\n",
    "\n",
    "Consider the hyperbolic cosine function\n",
    "\n",
    "$$\n",
    "z = \\cosh{x} = \\frac{e^x + e^{-x}}{2}\n",
    "$$\n",
    "\n",
    "First, just compute $\\frac{dz}{dx}$ using ordinary calculus.  It turns out this is the sinh function:\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\sinh{x} = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "Now, let's compute these numerically using backpropagation, done by hand.  Here is the computation graph for cosh:\n",
    "\n",
    "<img src=\"https://cs7150.baulab.info/2022-Fall/hw2/cosh-backprop.png\">\n",
    "\n",
    "First, fill in the forward pass.  Compute and fill in the following values in order.  You can write the answers in terms of `log` where it is cleaner, e.g., `log 0.5` or `log 2`, etc.   (Tip: it is much easier to work out the forward and backward pass by using a pencil and paper by drawing the graph and filling it out; then you can copy your answers here)\n",
    "\n",
    "$$\n",
    "a = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "e = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "Now, fill in the backward pass, in the reverse order.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial z} = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial e} = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial d} = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial c} = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial b} = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial a} = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\boxed{\\text{TODO: fill me in}}\n",
    "$$\n",
    "\n",
    "Check your final answer for $\\frac{\\partial z}{\\partial x}$ against the value of sinh.  It should match.\n",
    "\n",
    "The \"copy\" operation occurs frequently but is typically not explicitly represented as a computation node in code.  Instead, we see it appear as *gradient accumulation*, where multiple backward paths leading to the same node provide gradient that is summed and accumulated over all the paths.  That is the case in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9797fc",
   "metadata": {},
   "source": [
    "## Exercise 2.0.2: implement backpropagation in code\n",
    "\n",
    "The `ComputationNode` base class below provides a starting point for implementing the backpropagation algorithm.  A computation graph is created by connecting a set of `ComputationNodes`, culminating in a single output.  Every node records:\n",
    "\n",
    "* A list of any number of input edges connecting to input nodes, connected at construction (`self.input_nodes`)\n",
    "* A single output, where the result will be stored as a number computed in the forward pass (`self.result`)\n",
    "* A single gradient, which is the accumulated upstream gradient computed in the backward pass (`self.gradient`)\n",
    "\n",
    "Because a computation graph ends at a single output which is recursively connected to the result of the graph through the input edges, the whole graph can be represented by a reference to the output node.\n",
    "\n",
    "The `ComputationNode` base class itself does not know how to compute any specific mathematical operation.  Each separate math operation will need a subclass.  `Leaf`, `Negate`, `Exp`, and `Mean` are provided as examples.\n",
    "\n",
    "Each node also comes with convenience methods for traversing the computation graph that ends at the given node:\n",
    "\n",
    "* `sorted_nodes()` returns a list of all the computation nodes in toplogically sorted order, output last.\n",
    "* `get(name)` finds the single node matching the given name.\n",
    "\n",
    "In addition, similar to pytorch, the base class provides several convenience methods to compute the negation, exponent, etc, by constructing computation nodes that take the current node as input.\n",
    "\n",
    "Read and familiarize yourself with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the code in this cell.\n",
    "import math\n",
    "\n",
    "class ComputationNode:\n",
    "    def __init__(self, *input_nodes, name=None):\n",
    "        '''\n",
    "        A computation graph is an acyclic graph of nodes, each representing\n",
    "        a value that is computed from its child (predecessor) input_nodes.\n",
    "        In our implementation, you must give the input nodes at construction time,\n",
    "        and we do not compute the result or the gradient right away.\n",
    "        That is your job.\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.input_nodes = [n if isinstance(n, ComputationNode) else Leaf(n)\n",
    "                            for n in input_nodes]\n",
    "        self.result = None\n",
    "        self.gradient = None\n",
    "    def forward(self):\n",
    "        '''One step of the forward pass will fill in self.result.'''\n",
    "        assert 'Subclass implementation needed'\n",
    "        self.result = function_of(self.input(0))\n",
    "    def backward(self):\n",
    "        '''One step of the backward pass will combine upstream `self.gradient` with\n",
    "        \"local gradients\" to return a list of \"downstream gradients\", one for each input.'''\n",
    "        assert 'Subclass implementation needed'\n",
    "        return [compute_downstream_gradient(upstream_gradient, self.result, self.input(i))\n",
    "                for i in range(len(self.input_nodes))]\n",
    "    def accumulate_gradient(self, upstream_gradient):\n",
    "        '''Each downstream gradient should be accumulated as an upstream gradient.'''\n",
    "        if self.gradient is None:\n",
    "            self.gradient = upstream_gradient\n",
    "        else:\n",
    "            self.gradient = self.gradient + upstream_gradient\n",
    "    def sorted_nodes(self, seen=None):\n",
    "        '''\n",
    "        Returns a list of all the nodes in the computation graph, in topological\n",
    "        sort order, with all inputs listed before the outputs that depend on them.\n",
    "        '''\n",
    "        if seen is None: seen = set()\n",
    "        nodes = []\n",
    "        if self not in seen: # Do not process the same node twice\n",
    "            seen.add(self)\n",
    "            for node in self.input_nodes:\n",
    "                nodes.extend(node.sorted_nodes(seen=seen))\n",
    "            nodes.append(self)\n",
    "        return nodes\n",
    "    def input(self, i):\n",
    "        '''Gets the value of the ith input, after it is computed.'''\n",
    "        if _building_graph:\n",
    "            return self.input_nodes[i]\n",
    "        else:\n",
    "            return self.input_nodes[i].result\n",
    "    @property\n",
    "    def shape(self):\n",
    "        '''\n",
    "        Returns the shape of the result, if any.\n",
    "        '''\n",
    "        return getattr(self.result, 'shape', ())\n",
    "    def __repr__(self):\n",
    "        '''Print out the node and its inputs recursively so you can see the graph.'''\n",
    "        with printing_graph() as seen:\n",
    "            name = type(self).__name__ + (' ' + self.name if self.name else '')\n",
    "            if self in seen: return f'{name} @[{seen[self]}]'\n",
    "            else: seen[self] = str(len(seen) + 1)\n",
    "            our_repr = f'{name} result={self.result} gradient={self.gradient}'\n",
    "            tree_repr = '\\n'.join([our_repr] + list(repr(node) for node in self.input_nodes))\n",
    "            return '\\n  '.join(tree_repr.split('\\n'))\n",
    "    \n",
    "    # Convenience methods.\n",
    "    def exp(self):\n",
    "        return Exp(self)\n",
    "    def sum(self):\n",
    "        return Matsum(self)\n",
    "    def expand(self, target_shape):\n",
    "        return Matexpand(self, target_shape)\n",
    "    def __neg__(self):\n",
    "        return Negate(self)\n",
    "    def __pow__(self, int_power):\n",
    "        return IntPow(self, int_power)\n",
    "    def __add__(self, other):\n",
    "        return Add(self, other) # You should make a Add class\n",
    "    def __radd__(self, other):\n",
    "        return Add(other, self) # You should make a Add class\n",
    "    def __sub__(self, other):\n",
    "        return Add(self, -other) # You should make a Add class\n",
    "    def __rsub__(self, other):\n",
    "        return Add(other, -self) # You should make a Add class\n",
    "    def __mul__(self, other):\n",
    "        return Mul(self, other) # You should make a Mul class\n",
    "    def __rmul__(self, other):\n",
    "        return Mul(other, self) # You should make a Mul class\n",
    "    def __truediv__(self, other):\n",
    "        return Mul(self, other ** -1) # You should make a Mul class\n",
    "    def __matmul__(self, other):\n",
    "        return Matmul(self, other) # You should make a Matmul class\n",
    "\n",
    "class Leaf(ComputationNode):\n",
    "    def __init__(self, value, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.value = value\n",
    "    def forward(self):\n",
    "        self.result = self.value\n",
    "    def backward(self):\n",
    "        return [] # no inputs, no downstream gradient\n",
    "\n",
    "class Negate(ComputationNode):\n",
    "    def forward(self):\n",
    "        self.result = -self.input(0)\n",
    "    def backward(self):\n",
    "        # return downstream gradient\n",
    "        return [-self.gradient]\n",
    "\n",
    "class Exp(ComputationNode):\n",
    "    def forward(self):\n",
    "        x = self.input(0)\n",
    "        self.result = x.exp() if callable(getattr(x, 'exp', None)) else math.exp(x)\n",
    "    def backward(self):\n",
    "        local_gradient = self.result\n",
    "        # return downstream gradient\n",
    "        return [local_gradient * self.gradient]\n",
    "\n",
    "class Mean(ComputationNode):\n",
    "    def forward(self):\n",
    "        self.result = (self.input(0) + self.input(1)) / 2\n",
    "    def backward(self):\n",
    "        # return downstream gradient for each of two inputs\n",
    "        return [self.gradient / 2, self.gradient / 2]\n",
    "\n",
    "class IntPow(ComputationNode):\n",
    "    def __init__(self, base, int_power):\n",
    "        super().__init__(base)\n",
    "        assert isinstance(int_power, int), 'Handles only constant integer powers'\n",
    "        # Note: you could rewrite this node to remove this restriction if you like.\n",
    "        self.int_power = int_power\n",
    "    def forward(self):\n",
    "        self.result = self.input(0) ** self.int_power\n",
    "    def backward(self):\n",
    "        # return downstream gradient\n",
    "        return [self.gradient * self.int_power * (self.input(0) ** (self.int_power - 1))]\n",
    "\n",
    "class Matsum(ComputationNode): # Support for tensor.sum() to reduce a tensor to a scalar\n",
    "    def forward(self):\n",
    "        self.result = self.input(0).sum()\n",
    "    def backward(self):\n",
    "        # return downstream gradient\n",
    "        return [self.gradient.expand(self.input(0).shape)]\n",
    "    \n",
    "class Matexpand(ComputationNode): # Support for tensor.expand() to expand a scalar to a tensor\n",
    "    def __init__(self, inp, target_shape):\n",
    "        super().__init__(inp)\n",
    "        self.target_shape = target_shape\n",
    "    def forward(self):\n",
    "        self.result = self.input(0).expand(self.target_shape)\n",
    "    def backward(self):\n",
    "        # return downstream gradient\n",
    "        return [self.gradient.sum()]\n",
    "\n",
    "# This flag will be used for higher-order derivatives in an extra-credit part of this exercise.\n",
    "from contextlib import contextmanager\n",
    "_building_graph = False\n",
    "_printing_graph = None\n",
    "@contextmanager\n",
    "def build_graph(build_graph=True):\n",
    "    global _building_graph\n",
    "    old = _building_graph\n",
    "    _building_graph = build_graph\n",
    "    try:\n",
    "        yield old\n",
    "    finally:\n",
    "        _building_graph = old\n",
    "@contextmanager\n",
    "def printing_graph():\n",
    "    global _printing_graph\n",
    "    old = _printing_graph\n",
    "    if old is None:\n",
    "        _printing_graph = {}\n",
    "    try:\n",
    "        yield _printing_graph\n",
    "    finally:\n",
    "        _printing_graph = old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba36fd",
   "metadata": {},
   "source": [
    "The code below is not finished, and your job is to fill it in with a correct implementation of the backpropagation algorithm.\n",
    "\n",
    "Below we will give you six successively more diffcult tests to pass, to get the code right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CODE\n",
    "# TODO: you should modify and add code below to implement the backpropagation algorithm.\n",
    "# All the code you should need to add will be within these classes and functions.\n",
    "\n",
    "class Mul(ComputationNode):\n",
    "    def forward(self):\n",
    "        # TODO: Add your code here\n",
    "        self.result = 0\n",
    "    def backward(self):\n",
    "        # TODO: Add your code here\n",
    "        return []\n",
    "class Add(ComputationNode):\n",
    "    def forward(self):\n",
    "        # TODO: Add your code here\n",
    "        self.result = 0\n",
    "    def backward(self):\n",
    "        # TODO: Add your code here\n",
    "        return []\n",
    "class Matmul(ComputationNode):\n",
    "    def forward(self):\n",
    "        # TODO: Add your code here\n",
    "        self.result = 0\n",
    "    def backward(self):\n",
    "        # TODO: Add your code here\n",
    "        return []\n",
    "\n",
    "def forward_algorithm(graph):\n",
    "    # TODO: Add your code here\n",
    "    return graph\n",
    "\n",
    "def backward_algorithm(graph, one=1.0):\n",
    "    # TODO: Add your code here\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a7572",
   "metadata": {},
   "source": [
    "### Step 1, compute the forward pass\n",
    "\n",
    "The following code constructs and prints the computation graph corresponding to the by-hand Exercise 1.  It only uses computation node classes that have already been implemented.\n",
    "\n",
    "It also runs the `forward_algorithm` which should compute the result of the formula.\n",
    "\n",
    "However, `forward_algorithm` is not yet implemented.\n",
    "\n",
    "**Your exercise:** implement `forward_algorithm` in the `MAIN CODE` cell above, and then verify that the `sinh_x.result` printed in the sample below is correct.  Your code should pass through all the nodes in the graph in the appropriate order and invoke the `forward` method on them.\n",
    "\n",
    "Once you have your algorithm complete, verify that the `cosh_x.result` printed in the sample below is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the code in this cell.\n",
    "x = Leaf(math.log(2), 'x')\n",
    "cosh_x = Mean(Exp(x), Exp(-x))\n",
    "print(forward_algorithm(cosh_x))\n",
    "print(cosh_x.result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e825e0",
   "metadata": {},
   "source": [
    "### Step 2, compute the backward pass\n",
    "\n",
    "This is the tricky part.  Your job is to implement the `backward_algorithm` in the `MAIN CODE` cell above and then verify that the code below works.\n",
    "\n",
    "Remember that your backward algorithm should:\n",
    " * process all nodes in the reverse order as the `forward_algorithm`\n",
    " * start by placing 1.0 in the final node's gradient.\n",
    " * accumulate every downstream gradient as an upstream gradient for each coresponding input node.  You can use `input_node.accumulate_gradient(g)` to do this.\n",
    "\n",
    "Once you have your algorithm complete, verify that the `x.gradient` printed in the sample below is correct.  Note that the derivative of cosh is sinh, which can be used to tell you what answer you should be getting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf02466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the code in this cell.\n",
    "print(backward_algorithm(cosh_x))\n",
    "print(x.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce770621",
   "metadata": {},
   "source": [
    "### Step 3, implement Add and Mul, and verify things work with reused internal nodes\n",
    "\n",
    "The function tanh is defined as:\n",
    "\n",
    "$$z = \\tanh x = \\frac{\\sinh x}{\\cosh x} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "In the cell above, if you implement `Add` and `Mul`, you should be able to make the following test code work.  You should not need to change any of the code in this cell.\n",
    "\n",
    "Notice when you build the graph, the division $a/b$ is interpreted as $a * b^{-1}$, and the integer power $b{^-1}$ is already provided for you; you just need to provide `Mul`.  Similarly $a-b$ is interpreted as $a + (-b)$ and the negation $-b$ is already provided; you just need to provide `Add`.\n",
    "\n",
    "The final subtlety is that computation graph has been cleverly arranged to avoid re-computing `exp(x)` and `exp(-x)` more than necessary.  The resuse of these terms can lead to erroneous backpropagation if your algorithm re-visits nodes or edges too many times.  If the previous test worked but you are getting the wrong answer on this one, check that each node and edge in the graph is being visited exactly once.\n",
    "\n",
    "When are are done, verify that your result is correct.  Remember from class that $\\frac{dz}{dx} = 1 - z^2$ in the tanh case.  Check this result against the results of your code below, and if it does not match, fix your algorithm.  Once everything is right, you should not need to change anything in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the code in this cell.\n",
    "x = Leaf(math.log(2), 'x')\n",
    "expx = Exp(x)\n",
    "expnx = Exp(-x)\n",
    "tanh_x = (expx - expnx) / (expx + expnx)\n",
    "forward_algorithm(tanh_x)\n",
    "backward_algorithm(tanh_x)\n",
    "print(tanh_x)\n",
    "print(x.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e98062",
   "metadata": {},
   "source": [
    "### Step 4, differentiate the polynomial from HW1\n",
    "\n",
    "Now that you have `Add` and `Mul` you should be able to handle the polynomials like the one we differentiated in  HW1, but to make nice plces we will first need to handle tensors rather than just python scalars.\n",
    "\n",
    "To process gradients over tensors rather than just derivatives over python scalars you will need one other change.  Modify your `backward_algorithm` so that instead of starting at `1.0` it can start at `torch.tensor(1.0)`.  The code below uses the `one` argument to select the option that we want, passing in `one=torch.tensor(1.0)`, which the `backward_algorithm` implementation should use for the initial gradient.\n",
    "\n",
    "In principle, everything should then work.\n",
    "\n",
    "Note that we have already done a bit of the magic for you: the code in the first cell has already provided a completed implementation of `Matsum` which is used below when we say `yy.sum()`.\n",
    "\n",
    "Once you have your implementation above correct, you should not need to change the test code below; it should plot the polynomial and its first derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the code in this cell.\n",
    "import torch\n",
    "\n",
    "def polynomial(x):\n",
    "    return x**4 - 2 * x**3 + 3 * x**2 - 4 * x + 5\n",
    "\n",
    "xx = Leaf(torch.linspace(-2.0, 3.0, 25), 'x')\n",
    "yy = polynomial(xx)\n",
    "ysum = yy.sum()\n",
    "forward_algorithm(ysum)\n",
    "backward_algorithm(ysum, one=torch.tensor(1.0))\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(xx.value, yy.result, label='y')\n",
    "plt.plot(xx.value, xx.gradient, label='dy/dx')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8af08e",
   "metadata": {},
   "source": [
    "### Step 5 (extra credit), reproduce higher-order gradients from HW1\n",
    "\n",
    "Your code might already do everything needed to make the higher-order derviatives in the code below work, which just passes `one=Leaf(torch.tensor(1.0))` to do the trick of building a computation graph for the gradients.  But you may be missing some details.\n",
    "\n",
    "For extra credit, make higher-order derivatives work by getting those details right:\n",
    "\n",
    "  * You will need to zero the gradients in the whole tree at the beginning of your backward_algorithm, since the code will be repeating the backward algorithm.  Before starting its main work, your backward_algorithm can just pass over all the graph nodes and set `node.gradient = None`.\n",
    "  * Whenever you compute results, you will need to the ability to return computation graph nodes that can be further differentiated instead of plain numerical values.  You will want to do this when you are running code within the `with build_graph()` context manager.   Some code to do that has already been done in the provided `input` method in the code above; it may be enough, or you may need to add more adjustements to your code.\n",
    "  * You will need to make sure any methods you call on numbers or tensors also work on nodes.  Several of the provided methods have already been written to handle this case; it may be enough, but you may need to make adjustments.\n",
    "\n",
    "Without making significant changes to your implementation beyond the details above, the code below should work.\n",
    "\n",
    "You should not need to change any code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e380f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the code in this cell.\n",
    "# (extra credit)\n",
    "xx = Leaf(torch.linspace(-2.0, 3.0, 25), 'x')\n",
    "yy = polynomial(xx)\n",
    "ysum = yy.sum()\n",
    "forward_algorithm(ysum)\n",
    "plt.plot(xx.value, yy.result, label='y')\n",
    "\n",
    "with build_graph():\n",
    "    backward_algorithm(ysum, one=Leaf(torch.tensor(1.0)))\n",
    "dydx = xx.gradient\n",
    "dysum = dydx.sum()\n",
    "forward_algorithm(dysum)\n",
    "plt.plot(xx.value, dydx.result, label='dydx')\n",
    "\n",
    "with build_graph():\n",
    "    backward_algorithm(dysum, one=Leaf(torch.tensor(1.0)))\n",
    "d2yd2x = xx.gradient\n",
    "d2ysum = d2yd2x.sum()\n",
    "forward_algorithm(d2ysum)\n",
    "plt.plot(xx.value, d2yd2x.result, label='d2yd2x')\n",
    "\n",
    "with build_graph():\n",
    "    backward_algorithm(d2ysum, one=Leaf(torch.tensor(1.0)))\n",
    "d3yd3x = xx.gradient\n",
    "d3ysum = d3yd3x.sum()\n",
    "forward_algorithm(d3ysum)\n",
    "plt.plot(xx.value, d3yd3x.result, label='d3yd3x')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da7f7f",
   "metadata": {},
   "source": [
    "### Step 6: Make matrix multiplication work.\n",
    "\n",
    "Taking gradients with respect to matrix multiplications is the bread and butter of neural network training: it is what we spend most of our GPU cycles doing.\n",
    "\n",
    "Fortunately, the computation is very similar to taking gradients of scalar products: just as `Mul` multiplies the upstream gradient by swapped inputs, `Matmul` need to matrix-multiply the upstream gradient by swapped inputs.  However, you will need to get the order of the multiplications correct, and you will need to transpose the inputs in the appropriate way.\n",
    "\n",
    "\n",
    "A few hints:\n",
    "  * Gradients with respect to tensors should always be the same shape as the results.\n",
    "  * If you look online for advice, be aware that there are two different conventions for matrix calculus notation, one which is the transpose of the other, and if you mix them up then you can get the wrong answers.  (They are called numerator versus denominator notation; the Wikipedia entry on matrix calculus explains the differences well.)  Of the two, denominator notation is more natural for backpropagation.\n",
    "  * If you are unsure, the safest way to get the right matrix operation is to expand the partial derivatives for a particular entry of the matrix by hand, and then work out which matrix multiplication is equivalent to the sum of products.\n",
    "  * The easiest way to test that you have the right matrix operation is to test the code with nonrectangular inputs, and make sure that the shapes are compatible; then to check the results against pytorch.\n",
    "\n",
    "Work through the linear algebra needed for computing partial derivatives of one or two specific entries of a matrix, and use your insights to implement `Matmul` in the `MAIN CODE` above by using matrix multiplications and transpose operations.  You should be able to pass the test below, which compares the gradients to the results given by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7158ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the code in this cell.\n",
    "\n",
    "A, B, C = [Leaf(torch.tensor(d, requires_grad=True), name) for name, d in [\n",
    "  ('A', [[1.0, 1.0], [0.0, 1.0]]),\n",
    "  ('B', [[1.0, 2.0, 1.0], [0.0, 1.0, 1.0]]),\n",
    "  ('C', [[3.0, 0.0, 1.0], [1.0, 2.0, 3.0]]),\n",
    "]]\n",
    "loss = ((A @ B - C) ** 2).sum()\n",
    "\n",
    "# Use our algorithm, with pytorch autograd disabled during the backward pass.\n",
    "forward_algorithm(loss)\n",
    "with torch.no_grad():\n",
    "    backward_algorithm(loss, one=torch.tensor(1.0))\n",
    "\n",
    "# Use pytorch autograd.\n",
    "loss.result.backward()\n",
    "\n",
    "# Print and compare the results.\n",
    "import re\n",
    "def despace(s):\n",
    "    return re.sub(r\"\\s*\", \"\", str(s))\n",
    "for name, param in zip('ABC', [A, B, C]):\n",
    "    print(f'pytorch says: dloss/d{name} = {despace(param.value.grad)}')\n",
    "    print(f'our code says dloss/d{name} = {despace(param.gradient)}')\n",
    "    print(f'difference: {(param.value.grad - param.gradient).norm()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2f584",
   "metadata": {},
   "source": [
    "In practice, we will not implement backpropagation by hand.\n",
    "\n",
    "Instead, we will use the autograd functionality built in to pytorch.  In pytorch, the `Tensor` objects that have set `requires_grad=True` play the role of our `ComputationNode`.  Forward computations are done immediately, and backward passes are done using `tensor.backward()` or `autograd.grad()`.  Pytorch extends the backpropagation techniques we have explored here to cover backprop through every operation that it can do on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8710541",
   "metadata": {},
   "source": [
    "## Exercise 2.0.3: calculate the typical time and memory use of backprop\n",
    "\n",
    "The following computation graph is a typical form for a deep neural network with $n$ layers, where the weights $W_i\\in\\mathbb{R}^{d\\times d}$ are the parameters of the network, and the data $x_i\\in\\mathbb{R}^{d\\times b}$ are the input and activation data that passes through the network as it computes a single batch of input.\n",
    "\n",
    "<img src=\"typical-nn-bp-graph.png\">\n",
    "\n",
    "Suppose each each batch of data $x_i$ gathers together $b$ $d$-dimensional vectors, i.e., it can be stored as a $d\\times b$ dimensional matrix.  Similarly each batch of inermediate results $z_i$ will be $d \\times b$ in size.  And suppose each network weight matrix $W_i$ is a $d\\times d$ dimensional matrix.\n",
    "\n",
    "You will count up the time and space complexity of forward and backward passes.  You should answer the following four questions in big-O notation with the correct asymptotic behavior of $n$, $b$ and $d$ as they grow large (in other words, you can discard any constant factor independent of those four variables, and you can disregard terms that are dominated by terms that will always grow larger).  However, do not assume that any particular one of $n$ or $b$ or $d$ are largest; we want formulas that are asymptotically correct if any of them happens to be much larger than the others.\n",
    "\n",
    "### Time complexity of ordinary backpropagation\n",
    "\n",
    "First let us count $T$, the number of scalar computation operations it takes to run the forward pass or backward pass through the network.  Recall that computing $z_i = W_i x_i$ is an ordinary matrix multiplication which uses:\n",
    "\n",
    "$$T_{\\text{mm}} = O(b d^2)$$\n",
    "\n",
    "Computing a multiplication such as $x_i z_i^T$ (muliplying a $d\\times b$ by a $b\\times d$ matrix to obtain a $d\\times d$ result) also uses the same number of operations $T_{\\text{mm}}$.\n",
    "\n",
    "And you can assume that both the forward pass and backward pass through a single $\\sigma$ or $f$ node is proportional to the input data size:\n",
    "\n",
    "$$T_{\\sigma} = O(bd)$$\n",
    "\n",
    "  * How many scalar operations does it take to run the network in inference (compute $\\mathcal{L}$ from $x_1$) without training, if gradients will not be needed?\n",
    "  \n",
    "$$T_{\\text{run}} = \\boxed{O(\\text{TODO: fill me in})}$$\n",
    "    \n",
    "\n",
    "  * What if gradients might be needed, how many scalar operations does it take to run the forward pass over thr network -- is it different?\n",
    "  \n",
    "$$T_{\\text{forward}} = \\boxed{O(\\text{TODO: fill me in})}$$\n",
    "\n",
    "\n",
    "\n",
    "  * How many scalar operations will it take to run backpropagation to get gradients ${\\partial{\\mathcal{L}}}/{\\partial{W_i}}$ on all network weights?\n",
    "\n",
    "$$T_{\\text{backward}} =\\boxed{O(\\text{TODO: fill me in})}$$\n",
    "\n",
    "\n",
    "\n",
    "  * How many scalar operations will it take if we wish to run backpropagation to obtain only the gradient for the first layer weights $W_1$, i.e., we want ${\\partial{\\mathcal{L}}}/{\\partial{W_1}}$, but we do not wish to compute gradients for other weights $W_i$ for any of the other layers $i > 1$?  Does it change the asymptotic time-complexity of the backpropagation in big-O notation?\n",
    "\n",
    "$$T_{\\text{backward-for-}W_1} =\\boxed{O(\\text{TODO: fill me in})}$$\n",
    "\n",
    "\n",
    "\n",
    "### Space complexity of ordinary backpropagation\n",
    "\n",
    "\n",
    "We begin by observing that, without running the network at all and just holding all the weights, the amount of memory consumed to hold the unchanging parameters is:\n",
    "  \n",
    "$$M_{\\text{net}} = O(n d^2)$$\n",
    "\n",
    "In the questions below, suppose that $M_{\\text{net}}$ is allocated and fixed, and we are interested in estimating the additional memory needed to do computations on the network above the storage of its parameters.\n",
    "\n",
    "  * How much memory does it take to run the network in inference (compute $\\mathcal{L}$ from $x_1$) whn gradients will not be needed? (Do not include the $M_{\\text{net}}$ storage of the parameters themselves.)\n",
    "  \n",
    "$$M_{\\text{run}} = \\boxed{O(\\text{TODO: fill me in})}$$\n",
    "    \n",
    "\n",
    "\n",
    "  * How much memory does it take to run the forward pass (compute $\\mathcal{L}$ from $x_1$) while building the computation graph that would be needed needed for computing gradients in the future using backprop, but without the step of actually doing backprop?  (Again do not include the $M_{\\text{net}}$ storage of the parameters themselves.)\n",
    "  \n",
    "$$M_{\\text{forward}} = \\boxed{O(\\text{TODO: fill me in})}$$\n",
    "\n",
    "\n",
    "\n",
    "  * How much memory does it take to run the forward *and* backward passes to get gradients ${\\partial{\\mathcal{L}}}/{\\partial{W_i}}$ on all network weights?  Again, do not count the network parameter memory $M_{\\text{net}}$, but be sure to count the size of the newly computed gradients ${\\partial{\\mathcal{L}}}/{\\partial{W_i}}$.\n",
    "\n",
    "$$M_{\\text{backward}} =\\boxed{O(\\text{TODO: fill me in})}$$\n",
    "\n",
    "\n",
    "\n",
    "  * How much memory does it take to run the forward *and* backward passes to get gradients on just the very first layer weights $W_1$, i.e., ${\\partial{\\mathcal{L}}}/{\\partial{W_1}}$, if we do not wish to compute gradients for other weights $W_i$ for any of the other layers $i > 1$?  (Again do not include the original parameters $M_{\\text{net}}$ itself, but the output gradient should be counted.)\n",
    "\n",
    "$$M_{\\text{backward-for-}W_1} =\\boxed{O(\\text{TODO: fill me in})}$$\n",
    "\n",
    "### Lessons to learn from these exercises\n",
    "\n",
    "As you can see, backpropagation is efficient in terms of computation speed, but it obtains this speed by using potentially a lot of memory.  The amount of memory consumed by backpropagation can vary widely depending on $n$, $d$, $b$, the complexity of the computation graph, and exactly which gradients you wish to compute.  So in practice you should remember:\n",
    "\n",
    " * Disable autograd when you do not need gradients, because building computation graphs can consume a lot of memory.\n",
    " * Set `requires_grad=False` on any specific variables that do not need gradients, because that can also save memory.\n",
    " * You may be able to save a lot of memory by reducing your batch size $b$, depending on your model size (i.e., depending on $d$ and $n$).  Gradient accumulation can be used to accumulate larger batch gradients out of many small batches.\n",
    "\n",
    "If you are desparate for memory because backpropagation is using too much, it is possible to change the tradeoff and spend some additional computation to do backpropagation with less memory.  The method is called [gradient checkpointing](https://github.com/cybertronai/gradient-checkpointing), and it is available in both pytorch and tensorflow. [(Pytorch documentation here.)](https://pytorch.org/docs/stable/checkpoint.html)  The idea was proposed by [Griewank (1992)](https://papers.baulab.info/also/Griewank-1992.pdf), and it works by discarding some of the intermediate results of the forward pass, and recomputing them right before they are needed during the backward pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
