{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEd8bgt4Iozz"
      },
      "source": [
        "# Toeplitz Convolution in 1D and 2D: Padding, Shifts, and Parameter Counting\n",
        "\n",
        "In this notebook, we:\n",
        "1. Demonstrate 1D convolution with a **Toeplitz** matrix.\n",
        "2. Define and test **equivariance** over integer shifts.\n",
        "3. See how **padding** changes the output size and affects equivariance.\n",
        "4. Move to a **2D** case, expressed using flattening, and replicate the convolution via an explicitly filled `Linear` layer in **block Topelitz** form.\n",
        "5. Wrap up with an exercise on **parameter counts**:\n",
        "   - Full linear dimension,\n",
        "   - Nonzero Toeplitz entries,\n",
        "   - Actual number of kernel parameters.\n",
        "\n",
        "In this notebook we are doing no optimization, so we disable gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uem2Iz_YIoz0"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPZO4GznIoz1"
      },
      "source": [
        "## Part 1: 1D Convolution\n",
        "\n",
        "We’ll do an unpadded 1D convolution:\n",
        "- Input length = **10** (mostly zeros at edges).\n",
        "- A size-3 kernel.\n",
        "- **No bias** in the convolution.\n",
        "\n",
        "Given an input vector\n",
        "$$\n",
        "x = \\bigl[x_0,\\, x_1,\\, \\dots,\\, x_{N-1}\\bigr] \\in \\mathbb{R}^{N}\n",
        "$$\n",
        "\n",
        "and a kernel (filter)\n",
        "\n",
        "$$\n",
        "w = \\bigl[w_0,\\, w_1,\\, \\dots,\\, w_{K-1}\\bigr] \\in \\mathbb{R}^K,\n",
        "$$\n",
        "\n",
        "the unpadded 1D convolution produces an output\n",
        "\n",
        "$$\n",
        "y \\in \\mathbb{R}^{\\,N-K+1}\n",
        "$$\n",
        "\n",
        "whose $n$-th entry is:\n",
        "\n",
        "$$\n",
        "y_n \\;=\\; \\sum_{r=0}^{K-1} \\, w_r \\,\\cdot\\, x_{\\,n + r}\n",
        "\\quad \\text{for} \\quad n = 0,\\,1,\\,\\ldots,\\,N-K.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### 1.2 Creating and Filling the Conv1d Kernel\n",
        "\n",
        "Let’s define a small kernel, e.g.\\ $w = [1,\\,1,\\,-2]$ within a PyTorch `Conv1d` layer called `conv1d_layer`, and apply it to a small sequence of \\(N=10\\) data items in the vector `x_1d`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AC24JAKIoz1"
      },
      "source": [
        "# Our 1D input: shape (channels=1, length=10)\n",
        "x_1d = torch.tensor([[0.,0.,1.,2.,3.,4.,5.,6.,7.,0.]])\n",
        "print(\"x_1d shape:\", x_1d.shape)\n",
        "print(\"x_1d:\", x_1d)\n",
        "\n",
        "# Conv1d with kernel_size=3, out_channels=1, no bias.\n",
        "conv1d_layer = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
        "\n",
        "# Manually set the kernel to [1,1,-2]\n",
        "with torch.no_grad():\n",
        "    # We'll do something like conv1d_layer.weight[0,0] = torch.tensor([1.,2.,1.])\n",
        "    conv1d_layer.weight[0,0] = torch.tensor([1., 1., -2.])\n",
        "\n",
        "print(\"Conv1D kernel:\", conv1d_layer.weight)\n",
        "print()\n",
        "\n",
        "# Forward pass (valid convolution -> output length = 10-3+1 = 8)\n",
        "y_1d_conv = conv1d_layer(x_1d)\n",
        "print(\"Output shape:\", y_1d_conv.shape)\n",
        "print(\"Output:\", y_1d_conv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAa8nEnpIoz2"
      },
      "source": [
        "In the unpadded case, the output length is the input length minus the kernel size plus one.  In this case, that is:\n",
        "\n",
        "$$N - K + 1 = 10 - 3 + 1 = \\fbox{your answer}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Topelitz matrix equivalent\n",
        "\n",
        "A convolution is a linear operation, which means it is equivalent to multiplying by a single simple matrix.\n",
        "\n",
        "The needed form of the matrix is **Toeplitz matrix**, which is a matrix in which each descending diagonal from left to right is constant. For **1D convolution** with a kernel $w$ of length $K$, and an input of length $N$, the unpadded convolution output has length $N-K+1$. Denote these as:\n",
        "\n",
        "$$\n",
        "x = [x_0, x_1, x_2, \\dots, x_{N-1}]^T,\\quad\n",
        "w = [w_0, w_1, \\dots, w_{K-1}],\\quad\n",
        "y = [y_0, y_1, \\dots, y_{N-K}]^T.\n",
        "$$\n",
        "\n",
        "The 1D convolution is:\n",
        "$$\n",
        "y[n] = \\sum_{k=0}^{K-1} w_k \\cdot x_{n+k},\\quad n = 0,1,\\dots, N-K.\n",
        "$$\n",
        "We can encode this in a matrix multiplication:\n",
        "$$\n",
        "y = T\\,x,\n",
        "$$\n",
        "where \\(T\\) is a \\((N-K+1) \\times N\\) **Toeplitz** matrix of the form:\n",
        "$$\n",
        "T = \\begin{bmatrix}\n",
        "w_0 & w_1 & w_2 & \\dots & w_{K-1} & 0      & 0 & \\dots \\\\\n",
        "0   & w_0 & w_1 & \\dots & w_{K-2} & w_{K-1} & 0 & \\dots \\\\\n",
        "\\vdots &   &  & \\ddots &   &  &  &  \\\\\n",
        "0 & 0 & 0 & \\dots & w_0 & w_1 & \\dots & w_{K-1}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "All rows are **shifted** copies of the same kernel, implying **weight sharing**. A **generic** $(N-K+1) \\times N$ matrix would have $N-K+1 \\times N$ degrees of freedom, but Toeplitz structure means we only have **$K$ free parameters**.\n",
        "\n",
        "\n",
        "In our case, since the input is 10-dimesional and the output is 8-dimensonal,\n",
        "\n",
        "$$\n",
        "\\text{weight} \\in \\mathbb{R}^{8 \\times 10}, \\quad y = \\text{weight} \\cdot x.\n",
        "$$\n",
        "\n",
        "We only need **3** unique weights $(w_0, w_1, w_2)$ repeated across the rows.\n",
        "\n",
        "\n",
        "**Exercise**: fill in the Toeplitz matrix equivalent to the convolution in section 1.2."
      ],
      "metadata": {
        "id": "YAO-4eokN1FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layer = nn.Linear(in_features=10, out_features=8, bias=False)\n",
        "linear_layer.weight[...] = torch.Tensor([\n",
        "    ## PROVIDE YOUR ANSWERS HERE\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "]);\n",
        "\n",
        "print(\"Linear layer with parameters:\", linear_layer.weight.shape)\n",
        "print()\n",
        "\n",
        "y_linear = linear_layer(x_1d)\n",
        "print(\"Linear output shape:\", y_linear.shape)\n",
        "print(\"Linear output:      \", y_linear)\n",
        "\n",
        "print(\"Compare to conv out:\", y_1d_conv)\n",
        "print(\"Difference:\", F.mse_loss(y_1d_conv, y_linear).item())"
      ],
      "metadata": {
        "id": "nMPVvJOCQDC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all goes well, the difference should be near 0.\n",
        "\n",
        "### 1.2 Equivariance\n",
        "\n",
        "Let’s define a quick `make_shift(n)` function and test the equivariance of unpadded convolutions"
      ],
      "metadata": {
        "id": "v99jhrZBNxKz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vUu1cVEIoz2"
      },
      "source": [
        "def make_shift(n):\n",
        "    def shift_function(x):\n",
        "        \"\"\"\n",
        "        Shift the 1D tensor by n steps.\n",
        "        We'll keep the same length, so we lose data at one boundary\n",
        "        and add zeros at the other.\n",
        "        n>0 => shift right, n<0 => shift left.\n",
        "        \"\"\"\n",
        "        L = x.shape[-1]\n",
        "        zeros = torch.zeros_like(x[..., 0:abs(n)])\n",
        "        if n > 0:\n",
        "            return torch.cat([zeros, x[..., :-n]], dim=-1)\n",
        "        elif n < 0:\n",
        "            return torch.cat([x[..., -n:], zeros], dim=-1)\n",
        "        else:\n",
        "            return x\n",
        "    return shift_function\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function \\(f\\) is **equivariant** to a transformation $\\tau$ if:\n",
        "$$\n",
        "f(\\tau(x)) \\;=\\; \\tau\\bigl(f(x)\\bigr).\n",
        "$$\n",
        "\n",
        "In the case of **shift** in 1D, $\\tau$ can shift the data by $n$ steps, and equivariance means that when the input is shifted by some amount, the convolution output should be shifted accordingly.\n",
        "\n",
        "**Exercise:** Modify the following code to test equivariance with respect to various shifts and then report your findings."
      ],
      "metadata": {
        "id": "9Xe413hANFfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test of equivariance:\n",
        "tau = make_shift(1)\n",
        "\n",
        "print(\"Original input x_1d:  \", x_1d)\n",
        "print(\"Shifted input:        \", tau(x_1d))\n",
        "print(\"Shifted output:       \", tau(y_1d_conv))\n",
        "print(\"Conv of shifted input:\", (\"-your answer-\")) # FILL THIS IN\n"
      ],
      "metadata": {
        "id": "OgeopxQMMge9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpP3SOZ9Ioz2"
      },
      "source": [
        "Test a range of shifts from -5 to 5.\n",
        "\n",
        "Is the padded convolution equivariant with respect to $\\tau$?\n",
        "\n",
        "**Student**: After testing on this output, equivariance holds for this input when the shifts by $i$ are in the set: $\\fbox{your answer here}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVwX2AppIoz2"
      },
      "source": [
        "\n",
        "---\n",
        "## Part 2: Adding Padding=1 in 1D\n",
        "\n",
        "Now let's see how **padding** affects the output shape and equivariance. If we do `padding=1` with a `Conv1d`, a kernel of length=3, and input length=10, the output length is now larger (since the kernel can start at the left edge with a pad).\n",
        "\n",
        "### 2.1 Padded Conv1d\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIajeuSpIoz2"
      },
      "source": [
        "# We'll re-create a new conv1d for the padded case.\n",
        "conv1d_pad = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "conv1d_pad.weight[0,0] = torch.tensor([1.,1.,-2.])\n",
        "\n",
        "# This should produce an output of length=10.\n",
        "y_1d_pad = conv1d_pad(x_1d)\n",
        "print(\"Padding=1 -> output shape:\", y_1d_pad.shape)\n",
        "print(\"Output:\", y_1d_pad)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In padded case, the output length is the input length minus the kernel size plus twice the padding (assuming padding on both left and right), plus one.  In this case, that is:\n",
        "\n",
        "$$N - K + 2P + 1 = 10 - 3 + 2 + 1 = \\fbox{your answer}$$"
      ],
      "metadata": {
        "id": "qWe-8TxWWBG2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Co0PQXrIoz2"
      },
      "source": [
        "### 2.2 Toeplitz Matrix for the Padded Case\n",
        "\n",
        "Now the output is length=10, and the input is length=10, so we can do a `Linear(10,10, bias=False)` and fill it with the Toeplitz pattern for a 3-tap kernel **plus** the notion of 1 padding on each side. The Toeplitz approach for padding is effectively a 10×10 matrix with some of the corners hosting only a portion of the kernel.\n",
        "\n",
        "**Exercise**: Fill in the 10×10 so that it matches `padding=1` semantics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM_sRUmxIoz3"
      },
      "source": [
        "linear_layer_pad = nn.Linear(in_features=10, out_features=10, bias=False)\n",
        "linear_layer_pad.weight[...] = torch.Tensor([\n",
        "    ## PROVIDE YOUR ANSWERS HERE\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "]);\n",
        "\n",
        "print(\"Linear layer with parameters:\", linear_layer_pad.weight.shape)\n",
        "print()\n",
        "\n",
        "y_linear_pad = linear_layer_pad(x_1d)\n",
        "print(\"Linear output shape:\", y_linear_pad.shape)\n",
        "print(\"Linear output:      \", y_linear_pad)\n",
        "\n",
        "print(\"Compare to conv out:\", y_pad)\n",
        "print(\"Difference:\", F.mse_loss(y_1d_conv, y_linear).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the difference is near zero, we’ve matched the padded convolution.\n",
        "\n",
        "### 2.3 Check Equivariance with Padding=1\n",
        "We can again try shifting the input in the range \\(-5..5\\). With padding, the output is always length=10. But does that make it fully shift-equivariant?\n",
        "\n",
        "In practice, the *artificial* zeros at the edges might still cause boundary effects that break perfect equivariance once the shift is large enough.\n",
        "\n",
        "> **Student**: Test equivariance of the padded convolutoin and report your results here. Where does it break?"
      ],
      "metadata": {
        "id": "l14YnLbjXX4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test of equivariance:\n",
        "tau = make_shift(1)\n",
        "\n",
        "print(\"Original input x_1d:  \", x_1d)\n",
        "print(\"Shifted input:        \", tau(x_1d))\n",
        "print(\"Shifted output:       \", tau(y_1d_pad))\n",
        "print(\"Conv of shifted input:\", (\"-your answer-\")) # FILL THIS IN\n"
      ],
      "metadata": {
        "id": "omNxYyCxXTb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test a range of shifts from -5 to 5.\n",
        "\n",
        "Is the padded convolution equivariant with respect to $\\tau$?\n",
        "\n",
        "**Student**: After testing on this output, equivariance holds for this input when the shifts by $i$ are in the set: $\\fbox{your answer here}$."
      ],
      "metadata": {
        "id": "o8vhfRVoYgWP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK7nE4RKIoz3"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Part 3: 2D Convolution via Block-Toeplitz Matrices\n",
        "\n",
        "In 2D, the equivalent **Toeplitz** concept generalizes to **Block-Toeplitz** form. If your kernel is $R\\times S$, and the input image is $H\\times W$, then the valid convolution output is $(H - R + 1) \\times (W - S + 1)$. If you **flatten** the input image (length $H\\times W$) and flatten the output (length $(H - R + 1)(W - S + 1)$), you can create a big matrix $T$ of size $(H - R + 1)(W - S + 1) \\times (H\\times W)$. Each row in that matrix picks out **one** $R\\times S$ patch from the image.\n",
        "\n",
        "\n",
        "### 3.1 A 2d Binary Input Image\n",
        "Let’s create a 4×5 binary image, define a 3×3 kernel, and compare:\n",
        "1. **PyTorch** 2D convolution\n",
        "2. A fully connected (`Linear`) layer that we fill with the **block-Toeplitz** pattern.\n",
        "3. Confirm they match, then **visualize** both outputs.\n",
        "With 4×5 and  3×3 **unpadded** kernel, the convolution yields output shape 2×3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVewDFSWIoz3"
      },
      "source": [
        "img_2d = torch.tensor([\n",
        "    [1,1,0,1,0],\n",
        "    [0,1,1,0,1],\n",
        "    [1,0,0,1,0],\n",
        "    [1,1,1,0,0]\n",
        "], dtype=torch.float)\n",
        "\n",
        "print(\"Image shape: 4x5\")\n",
        "print(img_2d)\n",
        "plt.figure(figsize=(4,3))\n",
        "plt.imshow(img_2d, cmap='gray')\n",
        "plt.title(\"Binary Image (4x5)\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "x_2d = img_2d.unsqueeze(0).unsqueeze(0)  # (1,1,4,5)\n",
        "print(\"x_2d shape:\", x_2d.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_243x1zIoz3"
      },
      "source": [
        "### 3.2 Define a 3×3 Kernel and Convolve\n",
        "The valid output shape is (4-3+1)×(5-3+1) = 2×3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJaf3L5lIoz3"
      },
      "source": [
        "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
        "with torch.no_grad():\n",
        "    # Let's define some pattern for the 3x3 kernel:\n",
        "    conv2d.weight[0,0] = torch.tensor([\n",
        "        [ 2.,  1., -2.],\n",
        "        [-3., -1.,  1.],\n",
        "        [ 1.,  1., -1.]\n",
        "    ])\n",
        "\n",
        "y_2d = conv2d(x_2d)\n",
        "print(\"2D conv output shape:\", y_2d.shape)  # (1,1,2,3)\n",
        "print(\"Output:\", y_2d)\n",
        "\n",
        "plt.figure(figsize=(3,2))\n",
        "plt.imshow(y_2d[0,0,:,:], cmap='viridis')\n",
        "plt.title(\"Conv2d Output (2x3)\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IERtMbfiIoz3"
      },
      "source": [
        "### 3.3 Flattening the image and expressing the convolution as Toeplitz\n",
        "\n",
        "If we flatten the **4×5** input (2 elements) in **row-major** order and flatten the **2x3** output (6 elements).\n",
        "\n",
        "$$\n",
        "\\text{vec}(X) = [\\,x_{0,0},\\; x_{0,1},\\;\\dots,\\; x_{0,4},\\; x_{1,0},\\dots, x_{3,4}\\,]^\\top \\in \\mathbb{R}^{20}.\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{vec}(Y) = [\\,y_{0,0},\\; y_{0,1},\\; y_{0,2},\\; y_{1,0},\\; y_{1,1},\\; y_{1,2}\\,]^\\top \\in \\mathbb{R}^{6}.\n",
        "$$\n",
        "\n",
        "Then a fully connected layer from 20 to 6 can replicate the same operation if we fill it in a **Toeplitz** pattern of size 6x20.\n",
        "- Each row (in the 6) picks out a certain 3×3 patch from the 20.\n",
        "- The kernel’s 9 coefficients get placed in the right positions in that row.\n",
        "\n",
        "For example, in the example above, the first row would contain\n",
        "\n",
        "`[  2.,  1., -2.,  0.,  0., -3., -1.,  1.,  0.,  0.,  1.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]'\n",
        "`\n",
        "\n",
        "\n",
        "> So, of all the many parameters of `Linear(in_features=20, out_features=6, bias=False)`, we only only want **9** unique parameters repeated in positions that correspond to the same kernel value.\n",
        "\n",
        "### 3.4 Exercise: Fill a `Linear(20,6)` with the block Toeplitz Layout\n",
        "We create a `nn.Linear(20, 6, bias=False)`. Then for each output position `(r,c)`, we place the kernel into the correct 9 positions referencing `(r+dr, c+dc)` in the input.  To figure out this form, it is helpful to understand which row and which column in the full matrix corresponds to each 3x3 patch in the original image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pUwB7Y7Ioz3"
      },
      "source": [
        "fc2d = nn.Linear(in_features=20, out_features=6, bias=False)\n",
        "fc2d.weight[...] = torch.Tensor([\n",
        "    ## PROVIDE YOUR ANSWERS HERE\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "    [  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "]);\n",
        "\n",
        "# Multiply x_2d flattened\n",
        "x_2d_flat = x_2d.view(1,20)\n",
        "y_fc2d = fc2d(x_2d_flat)  # shape (1,6)\n",
        "y_fc2d_reshaped = y_fc2d.view(2,3)\n",
        "\n",
        "print(\"fc2d output (2x3):\\n\", y_fc2d_reshaped)\n",
        "diff_2d = y_fc2d_reshaped - y_2d[0,0]\n",
        "print(\"Difference vs conv2d:\", diff_2d)\n",
        "\n",
        "# Visual comparison\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(y_2d[0,0].detach(), cmap='viridis')\n",
        "plt.title(\"Conv2D Output\")\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(y_fc2d_reshaped.detach(), cmap='viridis')\n",
        "plt.title(\"Linear Block-Toeplitz Output\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ0kjsYcIoz3"
      },
      "source": [
        "If two operations look identical, we have successfully replicated the 2D convolution with a single large matrix that repeats the 9 kernel parameters in a block-Toeplitz pattern.\n",
        "\n",
        "---\n",
        "## Part 4: Parameter Counts\n",
        "\n",
        "Finally, let’s highlight the massive difference in parameter counts:\n",
        "\n",
        "**Student** fill in the counts.\n",
        "\n",
        "1. **Full Linear**: (out_features) × (in_features). The number of parameters in a the full linear operation going from 20 dimensions to 2 is $\\fbox{your answer}$.\n",
        "2. **Block-Toeplitz**: Each **non-zero** entry in the block Toeplitz matrix corresponds to a multiplication operation that needs to be done during convolution.  The number of nonzero entries in the block Toeplitz matrix above is $\\fbox{your answer}$.\n",
        "3. **Convolution Kernel**: Each entry block Toplitz matrix corresponds to one of the few **unique** parameters in the convolutional kernel.  The number of parameters in the convolutional kernel in this exercise was $\\fbox{your answer}$.\n",
        "\n",
        "Although a convolution is does not add any new computaional power beyond a linear operation, it provides advantages through **sparsity**, which dramatically reduces the amount of computation needed to process the input data, and through **weight sharing**, which dramatically reduces the size of the parameter space that needs to be searched by the optimization process.\n",
        "\n",
        "\n",
        "---\n",
        "## Conclusion\n",
        "\n",
        "We have now:\n",
        "- Demonstrated **Toeplitz** and **block-Toeplitz** construction for valid convolution in 1D and 2D.\n",
        "- Seen how **padding** changes the dimension and partial equivariance.\n",
        "- Highlighted the difference between a fully dense linear map and a convolution with **weight sharing**."
      ]
    }
  ],
  "metadata": {
    "name": "Toeplitz_Padding_Shifts_2D",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}